# -*- coding: utf-8 -*-
"""convert-hf2gguf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wpAtZURsi47yXM8BuI6SYh3mZH1wnEQz
"""

from huggingface_hub import snapshot_download
model_id="KasparZ/mtext-050625_mistral-7B-v0.3_merged"

#snapshot_download(repo_id=model_id, local_dir="mtext-050625_mistral-7B-v0.3_merged", local_dir_use_symlinks=False, revision="main")

#git clone https://github.com/ggerganov/llama.cpp.git # version b5474, publiée le 24 mai 2025

# convert to gguf (old)
!python llama.cpp/examples/convert_legacy_llama.py mtext-050625_mistral-7B-v0.3_merged \
  --outfile mtext-050625_mistral-7B-v0.3_merged.Q8_0.gguf \
  --outtype q8_0

# convert to gguf
!python llama.cpp/convert_hf_to_gguf.py mtext-141024_mistral-7B-v0.1_merged \
  --outfile mtext-141024_mistral-7B-v0.1_merged.gguf \
  --outtype q8_0

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import HfApi
api = HfApi()

model_id = "KasparZ/mtext-050625_mistral-7B-v0.3_merged"
api.create_repo(model_id, exist_ok=True, repo_type="model")
api.upload_file(
    path_or_fileobj="mtext-050625_mistral-7B-v0.3_merged.Q8_0.gguf",
    path_in_repo="mtext-050625_mistral-7B-v0.3_merged.Q8_0.gguf",
    repo_id=model_id,
)

# inférence
!./llama.cpp/llama-cli -m mtext-141024_mistral-7B-v0.1_merged2.gguf -p "<|s|>\n" -n 50